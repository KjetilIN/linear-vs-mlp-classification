{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN4050 Mandatory Assignment 2, 2024: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "Before you begin the exercise, review the rules at this website: https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html , in particular the paragraph on cooperation. This is an individual assignment. You are not allowed to deliver together or copy/share source-code/answers with others. Read also the \"Routines for handling suspicion of cheating and attempted cheating at the University of Oslo\": https://www.uio.no/english/studies/examinations/cheating/index.html By submitting this assignment, you confirm that you are familiar with the rules and the consequences of breaking them.\n",
    "\n",
    "### Delivery\n",
    "\n",
    "**Deadline**: Tuesday, October 29, 2024, 23:59\n",
    "\n",
    "Your submission should be delivered in Devilry. You may redeliver in Devilry before the deadline, but include all files in the last delivery, as only the last delivery will be read. You are recommended to upload preliminary versions hours (or days) before the final deadline.\n",
    "\n",
    "### What to deliver?\n",
    "\n",
    "You are recommended to solve the exercise in a Jupyter notebook, but you might solve it in a regular Python script if you prefer.\n",
    "\n",
    "#### Alternative 1\n",
    "If you prefer not to use notebooks, you should deliver the code, your run results, and a PDF report where you answer all the questions and explain your work.\n",
    "\n",
    "#### Alternative 2\n",
    "If you choose Jupyter, you should deliver the notebook. You should answer all questions and explain what you are doing in Markdown. Still, the code should be properly commented. The notebook should contain results of your runs. In addition, you should make a pdf of your solution which shows the results of the runs. (If you can't export: notebook -> latex -> pdf on your own machine, you may do this on the IFI linux machines.)\n",
    "\n",
    "Here is a list of *absolutely necessary* (but not sufficient) conditions to get the assignment marked as passed:\n",
    "\n",
    "- You must deliver your code (Python script or Jupyter notebook) you used to solve the assignment.\n",
    "- The code used for making the output and plots must be included in the assignment. \n",
    "- You must include example runs that clearly shows how to run all implemented functions and methods.\n",
    "- All the code (in notebook cells or python main-blocks) must run. If you have unfinished code that crashes, please comment it out and document what you think causes it to crash. \n",
    "- You must also deliver a pdf of the code, outputs, comments and plots as explained above.\n",
    "\n",
    "Your report/notebook should contain your name and username.\n",
    "\n",
    "Deliver one single compressed folder (.zip, .tgz or .tar.gz) which contains your complete solution.\n",
    "\n",
    "Important: if you weren’t able to finish the assignment, use the PDF report/Markdown to elaborate on what you’ve tried and what problems you encountered. Students who have made an effort and attempted all parts of the assignment will get a second chance even if they fail initially. This exercise will be graded PASS/FAIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of the assignment\n",
    "The goal of this assignment is to get a better understanding of supervised learning with gradient descent. It will, in particular, consider the similarities and differences between linear classifiers and multi-layer feed forward neural networks (multi-layer perceptrons, MLP) and the differences and similarities between binary and multi-class classification. A significant part is dedicated to implementing and understanding the backpropagation algorithm. \n",
    "\n",
    "### Tools\n",
    "The aim of the exercises is to give you a look inside the learning algorithms. You may freely use code from the weekly exercises and the published solutions. You should not use machine learning libraries like Scikit-Learn or PyTorch, because the point of this assignment is for you to implement things from scratch. You, however, are encouraged to use tools like NumPy and Pandas, which are not ML-specific.\n",
    "\n",
    "The given precode uses NumPy. You are recommended to use NumPy since it results in more compact code, but feel free to use pure Python if you prefer. \n",
    "\n",
    "### Beware\n",
    "This is a revised assignment compared to earlier years. If anything is unclear, do not hesitate to ask. Also, if you think some assumptions are missing, make your own and explain them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn # This is only to generate a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We start by making a synthetic dataset of 2000 instances and five classes, with 400 instances in each class. (See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html regarding how the data are generated.) We choose to use a synthetic dataset---and not a set of natural occuring data---because we are mostly interested in properties of the various learning algorithms, in particular the differences between linear classifiers and multi-layer neural networks together with the difference between binary and multi-class data. In addition, we would like a dataset with instances represented with only two numerical features, so that it is easy to visualize the data. It would be rather difficult (although not impossible) to find a real-world dataset of the same nature. Anyway, you surely can use the code in this assignment for training machine learning models on real-world datasets.\n",
    "\n",
    "When we are doing experiments in supervised learning, and the data are not already split into training and test sets, we should start by splitting the data. Sometimes there are natural ways to split the data, say training on data from one year and testing on data from a later year, but if that is not the case, we should shuffle the data randomly before splitting. (OK, that is not necessary with this particular synthetic data set, since it is already shuffled by default by Scikit-Learn, but that will not be the case with real-world data) We should split the data so that we keep the alignment between X (features) and t (class labels), which may be achieved by shuffling the indices. We split into 50% for training, 25% for validation, and 25% for final testing. The set for final testing *must not be used* till the end of the assignment in part 3.\n",
    "\n",
    "We fix the seed both for data set generation and for shuffling, so that we work on the same datasets when we rerun the experiments. This is done by the `random_state` argument and the `rng = np.random.RandomState(2024)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "X, t_multi = make_blobs(n_samples=[400, 400, 400, 400, 400], centers=[[0,1],[4,2],[8,1],[2,0],[6,0]], \n",
    "                  n_features=2, random_state=2024, cluster_std=[1.0, 2.0, 1.0, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "indices = np.arange(X.shape[0])\n",
    "rng = np.random.RandomState(2024)\n",
    "rng.shuffle(indices)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train, dev and test\n",
    "X_train = X[indices[:1000],:]\n",
    "X_val = X[indices[1000:1500],:]\n",
    "X_test = X[indices[1500:],:]\n",
    "t_multi_train = t_multi[indices[:1000]]\n",
    "t_multi_val = t_multi[indices[1000:1500]]\n",
    "t_multi_test = t_multi[indices[1500:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will  make a second dataset with only two classes by merging the existing labels in (X,t), so that `0`, `1` and `2` become the new `0` and `3` and `4` become the new `1`. Let's call the new set (X, t2). This will be a binary set.\n",
    "We now have two datasets:\n",
    "\n",
    "- Binary set: `(X, t2)`\n",
    "- Multi-class set: `(X, t_multi)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_train = t_multi_train >= 3\n",
    "t2_train = t2_train.astype('int')\n",
    "t2_val = (t_multi_val >= 3).astype('int')\n",
    "t2_test = (t_multi_test >= 3).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the two traning sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) # You may adjust the size\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=t_multi_train, s=10.0)\n",
    "plt.title(\"Multi-class set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=t2_train, s=10.0)\n",
    "plt.title(\"Binary set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear classifiers\n",
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that even the binary set (X, t2) is far from linearly separable, and we will explore how various classifiers are able to handle this. We start with linear regression with the Mean Squared Error (MSE) loss, although it is not the most widely used approach for classification tasks: but we are interested. You may make your own implementation from scratch or start with the solution to the weekly exercise set 7. \n",
    "We include it here with a little added flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X, bias):\n",
    "    \"\"\"X is a NxM matrix: N datapoints, M features\n",
    "    bias is a bias term, -1 or 1, or any other scalar. Use 0 for no bias\n",
    "    Return a Nx(M+1) matrix with added bias in position zero\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    biases = np.ones((N, 1)) * bias # Make a N*1 matrix of biases\n",
    "    # Concatenate the column of biases in front of the columns of X.\n",
    "    return np.concatenate((biases, X), axis  = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to all Numpy classifiers --- if any\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinRegClass(NumpyClassifier):\n",
    "\n",
    "    def __init__(self, bias=-1):\n",
    "        self.bias=bias\n",
    "    \n",
    "    def fit(self, X_train, t_train, lr = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a NxM matrix, N data points, M features\n",
    "        t_train is a vector of length N,\n",
    "        the target class values for the training data\n",
    "        lr is our learning rate\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias:\n",
    "            X_train = add_bias(X_train, self.bias)\n",
    "            \n",
    "        (N, M) = X_train.shape\n",
    "        \n",
    "        self.weights = weights = np.zeros(M)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # print(\"Epoch\", epoch)\n",
    "            weights -= lr / N *  X_train.T @ (X_train @ weights - t_train)      \n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"X is a KxM matrix for some K>=1\n",
    "        predict the value for each point in X\"\"\"\n",
    "        if self.bias:\n",
    "            X = add_bias(X, self.bias)\n",
    "        ys = X @ self.weights\n",
    "        return ys > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train and test a first classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, gold):\n",
    "    return np.mean(predicted == gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NumpyLinRegClass()\n",
    "cl.fit(X_train, t2_train, lr=0.1, epochs=100)\n",
    "print(\"Accuracy on the validation set:\", accuracy(cl.predict(X_val), t2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a small procedure which plots the data set together with the decision boundaries. \n",
    "You may modify the colors and the rest of the graphics as you like.\n",
    "The procedure will also work for multi-class classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, t, clf=[], size=(8,6)):\n",
    "    \"\"\"Plot the data set (X,t) together with the decision boundary of the classifier clf\"\"\"\n",
    "    # The region of the plane to consider determined by X\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Make a prediction of the whole region\n",
    "    h = 0.02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Classify each mesh point.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=size) # You may adjust this\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap = 'Paired')\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], c=t, s=10.0, cmap='Paired')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Decision regions\")\n",
    "    plt.xlabel(\"x0\")\n",
    "    plt.ylabel(\"x1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_train, t2_train, cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Tuning\n",
    "\n",
    "The result is far from impressive. \n",
    "Remember that a classifier which always chooses the majority class will have an accuracy of 0.6 on this data set.\n",
    "\n",
    "Your task is to try various settings for the two training hyper-parameters, learning rate and the number of epochs, to get the best accuracy on the validation set. \n",
    "\n",
    "Report how the accuracy varies with the hyper-parameter settings. It it not sufficient to give the final hyperparameters. You must also show how you found then and results for alternative values you tried out.\n",
    "\n",
    "When you are satisfied with the result, you may plot the decision boundaries, as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Here is the complete list of all tested hyper-parameters with the given accuracy:\n",
    "\n",
    "| Learning Rate | Epochs | Accuracy |\n",
    "|---------------|--------|----------|\n",
    "| 0.1        | 10     | 54.2%    |\n",
    "| 0.1        | 100     | 53.4%    |\n",
    "| 0.1        | 1000     | 53.4%    |\n",
    "| 0.01        | 10     | 47%    |\n",
    "| 0.01        | 100     | 56.6%    |\n",
    "| 0.01        | 1000     | 75%    |\n",
    "| 0.03        | 1000     | 76%    |\n",
    "| 0.02        | 2000     | 76%    |\n",
    "| 0.002        | 2000     | 67.4%    |\n",
    "| 0.002        | 10000     | 76%    |\n",
    "\n",
    "\n",
    "I started by trying the same learning rate with different epochs. Increasing the amount of epochs did not improve the overall accuracy. This makes sense, because more iterations with same learning rate will just go over the global optima with the same step. Also, with very few epochs we do underfitting by not allowing the model to adjust enough. \n",
    "\n",
    "Decreasing the learning rate allows us to approach the local optima. It requires more epochs, but the performance are improved. The best accuracy was when I had was **76%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model with parameters \n",
    "hyper_parameter_model = NumpyLinRegClass()\n",
    "hyper_parameter_model.fit(X_train, t2_train, lr=0.03, epochs=1000)\n",
    "print(\"Accuracy on the validation set:\", accuracy(hyper_parameter_model.predict(X_val), t2_val))\n",
    "\n",
    "# Plotting the decision boundary \n",
    "plot_decision_regions(X_train, t2_train, hyper_parameter_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the lectures that scaling the data may improve training speed and sometimes the performance. \n",
    "\n",
    "- Implement a scaler, at least the standard scaler (normalizer), but you can also try other techniques\n",
    "- Scale the data\n",
    "- Train the model on the scaled data\n",
    "- Experiment with hyper-parameter settings and see whether you can speed  up  the training.\n",
    "- Report final hyper-parameter settings and show how you found them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "For scaling the data, I implemented normalization which uses the minimum and maximum values as scale each data based on the upper and lower bound. It worked really well. The following was tested when trying to find a new hyper parameters:\n",
    "\n",
    "| Learning Rate | Epochs | Accuracy |\n",
    "|---------------|--------|----------|\n",
    "| 0.02        | 1000     | 64%    |\n",
    "| 0.03        | 1000     | 65.4%    |\n",
    "| 0.08        | 1000     | 73%    |\n",
    "| 0.2        | 1000     | 76%    |\n",
    "| 0.4        | 1000     | 77.2%    |\n",
    "| 0.44        | 1000     | 77.4%    |\n",
    "\n",
    "\n",
    "The best solution was **77.4%**. The overall performance got improved by scaling the data. It was able to perform better. \n",
    "The hyper parameter for the last solution was worse than the original hyper parameters, but the new one improved better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler function for normalizing the data\n",
    "def scaler(data):\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    return data\n",
    "\n",
    "# Scaling the training data\n",
    "X_scaled = scaler(X_train)\n",
    "t_scaled = scaler(t2_train)\n",
    "\n",
    "# Create the model with scaled data \n",
    "scaled_model = NumpyLinRegClass()\n",
    "scaled_model.fit(X_scaled, t_scaled, lr=0.44, epochs=1000)\n",
    "print(\"Accuracy on the validation set:\", accuracy(scaled_model.predict(X_val), t2_val))\n",
    "\n",
    "# Plotting the decision boundary \n",
    "plot_decision_regions(X_scaled, t_scaled, scaled_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "a) You should now implement a logistic regression classifier similarly to the classifier based on linear regression.\n",
    "You may use the code from the solution to weekly exercise set week07.\n",
    "\n",
    "b) In addition to the method `predict()` which predicts a class for the data, include a method `predict_probability()` which predict the probability of the data belonging to the positive class.\n",
    "\n",
    "c) So far, we have not calculated the loss explicitly in the code. Extend the code to calculate the loss on the training set for each epoch and to store the losses such that the losses can be inspected after training. The prefered loss for logistic regression is binary cross-entropy, but you can also try mean squared error. The most important is that your implementation of the loss corresponds to your implementation of the gradient descent.\n",
    "Also, calculate and store accuracies after each epoch.\n",
    "\n",
    "d) In addition, extend the `fit()` method with optional arguments for a validation set (X_val, t_val). If a validation set is included in the call to `fit()`, calculate the loss and the accuracy for the validation set after each epoch. \n",
    "\n",
    "e) The training runs for a number of epochs. We cannot know beforehand for how many epochs it is reasonable to run the training. One possibility is to run the training until the learning does not improve much. Extend the `fit()` method with two keyword arguments, `tol` (tolerance) and `n_epochs_no_update` and stop training when the loss has not improved with more than `tol` after `n_epochs_no_update`. A possible default value for `n_epochs_no_update` is 5. Also, add an attribute to the classifier which tells us after fitting how many epochs it was trained for.\n",
    "\n",
    "f) Train classifiers with various learning rates, and with varying values for `tol` for finding the optimal values. Also consider the effect of scaling the data.\n",
    "\n",
    "g) After a succesful training, for your best model, plot both training loss and validation loss as functions of the number of epochs in one figure, and both training and validation accuracies as functions of the number of epochs in another figure. Comment on what you see. Are the curves monotone? Is this as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for logistic function\n",
    "# It is also known as a sigmoid function\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "# Code for the logistic regression classifier \n",
    "class LogisticClassifier:\n",
    "    def __init__(self, bias=-1):\n",
    "        self.bias=bias\n",
    "        self.epochs_trained = 0\n",
    "\n",
    "\n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None, eta = 0.1, epochs=10, tol=1, n_epochs_no_update=5, logging=False):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "\n",
    "        # For task c)\n",
    "        # Store loss and accuracies\n",
    "        loss = []\n",
    "        accuracies = []\n",
    "        \n",
    "        # Add bias to training data \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train, self.bias)\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = weights = np.zeros(m+1)\n",
    "\n",
    "        # Attribute for keeping track of epochs trained\n",
    "        self.epochs_trained = 0\n",
    "\n",
    "        # Variable to keep track of how many epochs trained without an update in weights \n",
    "        no_update_counter = 0\n",
    "        \n",
    "        # Train over the given epochs \n",
    "        for _ in range(epochs):\n",
    "            \n",
    "            # Increment epochs count\n",
    "            self.epochs_trained += 1\n",
    "\n",
    "            # Store what the weights was before training \n",
    "            weights_before_update = np.copy(weights)\n",
    "\n",
    "            # Change the weights with update rule of gradient decent \n",
    "            weights -= eta / k *  X_train.T @ (self.forward(X_train) - t_train)    \n",
    "\n",
    "            # For task e) \n",
    "            # Check if the changes are less than the tolerance set\n",
    "            # We take the mean of the absolute value of the difference, and check if it is less than the tolerance set\n",
    "            if(np.mean(np.abs(weights_before_update - weights)) < tol):\n",
    "                no_update_counter += 1\n",
    "            else:\n",
    "                # No reason to change to counter \n",
    "                no_update_counter = 0\n",
    "\n",
    "            \n",
    "            # For task d)\n",
    "            if X_val is None or t_val is None:\n",
    "                # Calculate the loss with cross entropy on the  \n",
    "                current_loss = self.cross_entropy_loss(X_validation=X_train, t_validation=t_train) \n",
    "                loss.append(current_loss)\n",
    "\n",
    "                # Calculate the accuracy over time \n",
    "                current_accuracy = self.accuracy(X_train, t_train)\n",
    "                accuracies.append(current_accuracy)\n",
    "            else:\n",
    "                # Must check that both are not none\n",
    "                # If only one of the validation set is set something, then we will get an error\n",
    "                assert X_val is not None and t_val is not None, \"Both train and target validation set must be set\"\n",
    "\n",
    "                # Use validation set instead\n",
    "                # Calculate the loss with cross entropy on the  \n",
    "                current_loss = self.cross_entropy_loss(X_validation=X_val, t_validation=t_val) \n",
    "                loss.append(current_loss)\n",
    "\n",
    "                # Calculate the accuracy over time \n",
    "                current_accuracy = self.accuracy(X_val, t_val)\n",
    "                accuracies.append(current_accuracy)\n",
    "\n",
    "            if logging:\n",
    "                print(f\"EPOCH {self.epochs_trained}:        {current_loss} loss, {round(current_accuracy*100, 2)}% accuracy\" )\n",
    "\n",
    "            # Check if we exit early do to no update\n",
    "            if (no_update_counter == n_epochs_no_update):\n",
    "                if logging:\n",
    "                    print(f\"[INFO] No new change in weight for {n_epochs_no_update} epochs in a row\")\n",
    "                return loss, accuracies\n",
    "\n",
    "        # Return the loss and accuracies over time \n",
    "        return loss, accuracies\n",
    "    \n",
    "    # For task e) \n",
    "    # Get the amount of epochs trained \n",
    "    def get_epochs_trained(self):\n",
    "        return self.epochs_trained\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward method that will do a single forward pass\"\"\"\n",
    "        return logistic(X @ self.weights)\n",
    "    \n",
    "    def score(self, x):\n",
    "        \"\"\"Takes input and does a single forward pass\"\"\"\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        \"\"\"Does a prediction on the data based on \"\"\"\n",
    "        z = add_bias(x, self.bias)\n",
    "        score = self.forward(z)\n",
    "        return (score>threshold).astype('int')\n",
    "    \n",
    "    # Task b)\n",
    "    # Since the sigmoid function naturally outputs the probability that a class belongs to the given output \n",
    "    # we can just use the score as the probability\n",
    "    def predict_probability(self, x):\n",
    "        \"\"\"Predict the probability that the data belongs to the positive class\"\"\"\n",
    "        z = add_bias(x, self.bias)\n",
    "        probability = self.forward(z)\n",
    "        return probability\n",
    "    \n",
    "\n",
    "    # For task c) \n",
    "    # Formula from: https://en.wikipedia.org/wiki/Cross-entropy\n",
    "    def cross_entropy_loss(self, X_validation, t_validation):\n",
    "        \"\"\"Cross entropy loss function for calculating the current loss\"\"\"\n",
    "        X = add_bias(X_validation, self.bias)\n",
    "        y_hat = logistic(X@self.weights)\n",
    "        y = t_validation\n",
    "\n",
    "        # Return the mean of cross entropy loss for each of the datapoints\n",
    "        return np.mean(-y*np.log(y_hat)-(1-y)*np.log(1-y_hat))\n",
    "    \n",
    "    # For task c)\n",
    "    # Calculate the accuracy \n",
    "    def accuracy(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        return np.mean(score == t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:        0.6901047336149906 loss, 56.2% accuracy\n",
      "EPOCH 2:        0.6872754092530365 loss, 56.2% accuracy\n",
      "EPOCH 3:        0.6846362935547602 loss, 56.2% accuracy\n",
      "EPOCH 4:        0.682167105260839 loss, 56.2% accuracy\n",
      "EPOCH 5:        0.6798498999986778 loss, 56.2% accuracy\n",
      "EPOCH 6:        0.677668803604485 loss, 56.2% accuracy\n",
      "EPOCH 7:        0.6756097729986484 loss, 56.2% accuracy\n",
      "EPOCH 8:        0.6736603827206532 loss, 56.6% accuracy\n",
      "EPOCH 9:        0.6718096350551618 loss, 56.6% accuracy\n",
      "EPOCH 10:        0.6700477916314443 loss, 56.8% accuracy\n",
      "EPOCH 11:        0.6683662244143954 loss, 56.8% accuracy\n",
      "EPOCH 12:        0.6667572840970994 loss, 56.8% accuracy\n",
      "EPOCH 13:        0.66521418403045 loss, 57.0% accuracy\n",
      "EPOCH 14:        0.663730897968951 loss, 57.0% accuracy\n",
      "EPOCH 15:        0.6623020700625086 loss, 57.0% accuracy\n",
      "EPOCH 16:        0.6609229356742289 loss, 57.2% accuracy\n",
      "EPOCH 17:        0.65958925174909 loss, 57.2% accuracy\n",
      "EPOCH 18:        0.658297235594794 loss, 57.4% accuracy\n",
      "EPOCH 19:        0.6570435110624863 loss, 57.4% accuracy\n",
      "EPOCH 20:        0.655825061230561 loss, 57.6% accuracy\n",
      "EPOCH 21:        0.6546391867994096 loss, 57.6% accuracy\n",
      "EPOCH 22:        0.6534834694989515 loss, 57.6% accuracy\n",
      "EPOCH 23:        0.6523557398947424 loss, 57.6% accuracy\n",
      "EPOCH 24:        0.6512540490530592 loss, 57.6% accuracy\n",
      "EPOCH 25:        0.6501766435914212 loss, 57.6% accuracy\n",
      "EPOCH 26:        0.6491219436993079 loss, 57.8% accuracy\n",
      "EPOCH 27:        0.6480885237651725 loss, 57.8% accuracy\n",
      "EPOCH 28:        0.6470750952909716 loss, 58.0% accuracy\n",
      "EPOCH 29:        0.6460804918150133 loss, 58.2% accuracy\n",
      "EPOCH 30:        0.6451036555986293 loss, 58.4% accuracy\n",
      "EPOCH 31:        0.6441436258625602 loss, 58.4% accuracy\n",
      "EPOCH 32:        0.6431995283855289 loss, 58.4% accuracy\n",
      "EPOCH 33:        0.6422705663007416 loss, 58.4% accuracy\n",
      "EPOCH 34:        0.64135601194639 loss, 58.4% accuracy\n",
      "EPOCH 35:        0.6404551996440111 loss, 58.4% accuracy\n",
      "EPOCH 36:        0.6395675192941112 loss, 58.4% accuracy\n",
      "EPOCH 37:        0.6386924106920524 loss, 58.4% accuracy\n",
      "EPOCH 38:        0.6378293584790877 loss, 58.6% accuracy\n",
      "EPOCH 39:        0.6369778876538303 loss, 59.0% accuracy\n",
      "EPOCH 40:        0.6361375595785362 loss, 59.2% accuracy\n",
      "EPOCH 41:        0.635307968422545 loss, 59.2% accuracy\n",
      "EPOCH 42:        0.6344887379921915 loss, 59.2% accuracy\n",
      "EPOCH 43:        0.6336795189026048 loss, 59.2% accuracy\n",
      "EPOCH 44:        0.6328799860521581 loss, 59.2% accuracy\n",
      "EPOCH 45:        0.6320898363650322 loss, 59.4% accuracy\n",
      "EPOCH 46:        0.6313087867714471 loss, 59.4% accuracy\n",
      "EPOCH 47:        0.6305365723987421 loss, 59.2% accuracy\n",
      "EPOCH 48:        0.6297729449496416 loss, 59.4% accuracy\n",
      "EPOCH 49:        0.6290176712468251 loss, 59.6% accuracy\n",
      "EPOCH 50:        0.6282705319253634 loss, 59.6% accuracy\n",
      "EPOCH 51:        0.6275313202567336 loss, 59.6% accuracy\n",
      "EPOCH 52:        0.6267998410900084 loss, 59.8% accuracy\n",
      "EPOCH 53:        0.626075909897481 loss, 59.8% accuracy\n",
      "EPOCH 54:        0.625359351913451 loss, 60.4% accuracy\n",
      "EPOCH 55:        0.624650001356181 loss, 60.6% accuracy\n",
      "EPOCH 56:        0.6239477007241728 loss, 60.8% accuracy\n",
      "EPOCH 57:        0.623252300158914 loss, 60.8% accuracy\n",
      "EPOCH 58:        0.622563656867125 loss, 61.2% accuracy\n",
      "EPOCH 59:        0.6218816345963234 loss, 61.2% accuracy\n",
      "EPOCH 60:        0.6212061031582015 loss, 61.6% accuracy\n",
      "EPOCH 61:        0.6205369379949328 loss, 61.8% accuracy\n",
      "EPOCH 62:        0.6198740197840509 loss, 61.8% accuracy\n",
      "EPOCH 63:        0.6192172340780273 loss, 61.8% accuracy\n",
      "EPOCH 64:        0.6185664709750913 loss, 61.8% accuracy\n",
      "EPOCH 65:        0.6179216248182129 loss, 61.8% accuracy\n",
      "EPOCH 66:        0.6172825939194942 loss, 61.8% accuracy\n",
      "EPOCH 67:        0.6166492803075144 loss, 61.8% accuracy\n",
      "EPOCH 68:        0.6160215894954264 loss, 62.0% accuracy\n",
      "EPOCH 69:        0.6153994302678429 loss, 62.2% accuracy\n",
      "EPOCH 70:        0.6147827144847456 loss, 62.2% accuracy\n",
      "EPOCH 71:        0.6141713569008398 loss, 62.2% accuracy\n",
      "EPOCH 72:        0.6135652749989359 loss, 62.2% accuracy\n",
      "EPOCH 73:        0.6129643888360847 loss, 62.6% accuracy\n",
      "[INFO] No new change in weight for 5 epochs in a row\n"
     ]
    }
   ],
   "source": [
    "# For task F) testing and training \n",
    "log_classifier = LogisticClassifier()\n",
    "loss_over_time, accuracy_over_time = log_classifier.fit(X_train,t2_train, X_val, t2_val, eta=0.01, tol=0.001, epochs=10000, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task F) tested hyper parameters\n",
    "\n",
    "The following was tested: \n",
    "\n",
    "| Learning Rate | Tolerance   | Epochs Trained       | Loss                  | Accuracy |\n",
    "|---------------|-------------|----------------------|-----------------------|----------|\n",
    "| 0.1           | 0.1         |  5                   | 0.627174880890108     |  59.6%   |\n",
    "| 0.01          | 0.1         |  5                   | 0.6798498999986778    |  56.2%   |  \n",
    "| 0.1           | 0.01        |  12                  | 0.5884915312473303    |  67.8%   |\n",
    "| 0.01          | 0.01        |  5                   | 0.6798498999986778    |  56.2%   |\n",
    "| 0.01          | 0.01        |  5                   | 0.6798498999986778    |  56.2%   |\n",
    "| 0.03          | 0.2         |  5                   | 0.6617931126286112    |  57.0%   |\n",
    "| 0.03          | 0.002       |  62                  | 0.5669401952077701    |  72.6%   |\n",
    "| 0.02          | 0.002       |  39                  | 0.609923611717376     |  63.2%   |\n",
    "| 0.002         | 0.0001      |  1284                | 0.551142403265901     |  73.8%   |\n",
    "| 0.006         | 0.0001      |  1270                | 0.5098578096572891    |  75.2%   |\n",
    "| 0.01          | 0.0001      |  1146                | 0.5010869403243003    |  75.4%   |\n",
    "| 0.01          | 0.0002      |  647                 | 0.514379357842556     |  75.6%   |\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classifiers\n",
    "We turn to the task of classifying when there are more than two classes, and the task is to ascribe one class to each input. We will now use the set (X, t_multi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"One-vs-rest\" with logistic regression\n",
    "We saw in the lectures how a logistic regression classifier can be turned into a multi-class classifier using the one-vs-rest approach. We train one logistic regression classifier for each class. To predict the class of an item, we run all the binary classifiers and collect the probability score from each of them. We assign the class which ascribes the highest probability.\n",
    "\n",
    "Build such a classifier. Train the resulting classifier on (X_train, t_multi_train), test it on (X_val, t_multi_val), tune the hyper-parameters and report the accuracy.\n",
    "\n",
    "Also plot the decision boundaries for your best classifier similarly to the plots for the binary case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial logistic regression\n",
    "In the lectures, we contrasted the one-vs-rest approach with the multinomial logistic regression, also called softmax classifier. Implement also this classifier, tune the parameters, and compare the results to the one-vs-rest classifier. (Don't expect a large difference on a simple task like this.)\n",
    "\n",
    "Remember that this classifier uses softmax in the forward phase. For loss, it uses categorical cross-entropy loss. The loss has a somewhat simpler form than in the binary case. To calculate the gradient is a little more complicated. The actual gradient and update rule is simple, however, as long as you have calculated the forward values correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multi-layer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first non-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simple implementation of a multi-layer perceptron or feed-forward neural network.\n",
    "For now, it is quite restricted.\n",
    "There is only one hidden layer.\n",
    "It can only handle binary classification.\n",
    "In addition, it uses a simple final layer similar to the linear regression classifier above.\n",
    "One way to look at it is what happens when we add a hidden layer to the linear regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP class below misses the implementation of the `forward()` function. Your first task is to implement it. \n",
    "\n",
    "Remember that in the forward pass, we \"feed\" the input to the model, the model processes it and produces the output. The function should make use of the logistic activation function and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define the logistic function and its derivative:\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_diff(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBinaryLinRegClass(NumpyClassifier):\n",
    "    \"\"\"A multi-layer neural network with one hidden layer\"\"\"\n",
    "    \n",
    "    def __init__(self, bias=-1, dim_hidden = 6):\n",
    "        \"\"\"Initialize the hyperparameters\"\"\"\n",
    "        self.bias = bias\n",
    "        # Dimensionality of the hidden layer\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.activ = logistic\n",
    "        \n",
    "        self.activ_diff = logistic_diff\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"TODO: \n",
    "        Perform one forward step. \n",
    "        Return a pair consisting of the outputs of the hidden_layer\n",
    "        and the outputs on the final layer\"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        # return hidden_outs, outputs\n",
    "    \n",
    "    def fit(self, X_train, t_train, lr=0.001, epochs = 100):\n",
    "        \"\"\"Initialize the weights. Train *epochs* many epochs.\n",
    "        \n",
    "        X_train is a NxM matrix, N data points, M features\n",
    "        t_train is a vector of length N of targets values for the training data, \n",
    "        where the values are 0 or 1.\n",
    "        lr is the learning rate\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Turn t_train into a column vector, a N*1 matrix:\n",
    "        T_train = t_train.reshape(-1,1)\n",
    "            \n",
    "        dim_in = X_train.shape[1] \n",
    "        dim_out = T_train.shape[1]\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.weights1 = (np.random.rand(\n",
    "            dim_in + 1, \n",
    "            self.dim_hidden) * 2 - 1)/np.sqrt(dim_in)\n",
    "        self.weights2 = (np.random.rand(\n",
    "            self.dim_hidden+1, \n",
    "            dim_out) * 2 - 1)/np.sqrt(self.dim_hidden)\n",
    "        X_train_bias = add_bias(X_train, self.bias)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            # One epoch\n",
    "            # The forward step:\n",
    "            hidden_outs, outputs = self.forward(X_train_bias)\n",
    "            # The delta term on the output node:\n",
    "            out_deltas = (outputs - T_train)\n",
    "            # The delta terms at the output of the hidden layer:\n",
    "            hiddenout_diffs = out_deltas @ self.weights2.T\n",
    "            # The deltas at the input to the hidden layer:\n",
    "            hiddenact_deltas = (hiddenout_diffs[:, 1:] * \n",
    "                                self.activ_diff(hidden_outs[:, 1:]))  \n",
    "\n",
    "            # Update the weights:\n",
    "            self.weights2 -= self.lr * hidden_outs.T @ out_deltas\n",
    "            self.weights1 -= self.lr * X_train_bias.T @ hiddenact_deltas \n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class for the members of X\"\"\"\n",
    "        Z = add_bias(X, self.bias)\n",
    "        forw = self.forward(Z)[1]\n",
    "        score= forw[:, 0]\n",
    "        return (score > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implemented, this model can be used to make a non-linear classifier for the set `(X, t2)`. Experiment with settings for learning rate and epochs and see how good results you can get. \n",
    "Report results for various settings. Be prepared to train for a long time (but you can control it via the number of epochs and hidden size). \n",
    "\n",
    "Plot the training set together with the decision regions as in Part I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the MLP classifier\n",
    "You should now make changes to the classifier similarly to what you did with the logistic regression classifier in part 1.\n",
    "\n",
    "a) In addition to the `predict()` method, which predicts a class for the data, include the `predict_probability()` method which predict the probability of the data belonging to the positive class. The training should be based on these values, as with logistic regression.\n",
    "\n",
    "b) Calculate the loss and the accuracy after each epoch and store them for inspection after training.\n",
    "\n",
    "c) Extend the `fit()` method with optional arguments for a validation set `(X_val, t_val)`. If a validation set is included in the call to `fit()`, calculate the loss and the accuracy for the validation set after each epoch.\n",
    "\n",
    "d) Extend the `fit()` method with two keyword arguments, `tol` (tolerance) and `n_epochs_no_update` and stop training when the loss has not improved for more than `tol` after `n_epochs_no_update`. A possible default value for `n_epochs_no_update` is 5. Add an attribute to the classifier which tells us after fitting how many epochs it was trained on.\n",
    "\n",
    "e) Tune the hyper-parameters: `lr`, `tol` and `dim-hidden` (size of the hidden layer).\n",
    "Also, consider the effect of scaling the data.\n",
    "\n",
    "f) After a succesful training with the best setting for the hyper-parameters, plot both training loss and validation loss as functions of the number of epochs in one figure, and both training and validation accuracies as functions of the number of epochs in another figure. Comment on what you see.\n",
    "\n",
    "g) The MLP algorithm contains an element of non-determinism. Hence, train the classifier 10 times with the optimal hyper-parameters and report the mean and standard deviation of the accuracies over the 10 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use a feed-forward neural network for non-linear multi-class classfication and apply it to the set `(X, t_multi)`.\n",
    "\n",
    "Modify the network to become a multi-class classifier. As a sanity check of your implementation, you may apply it to `(X, t_2)` and see whether you get similar results as above.\n",
    "\n",
    "Train the resulting classifier on `(X_train, t_multi_train)`, test it on `(X_val, t_multi_val)`, tune the hyper-parameters and report the accuracy.\n",
    "\n",
    "Plot the decision boundaries for your best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Final testing\n",
    "We can now perform a final testing on the held-out test set we created in the beginning.\n",
    "\n",
    "## Binary task (X, t2)\n",
    "Consider the linear regression classifier, the logistic regression classifier and the multi-layer network with the best settings you found. Train each of them on the training set and evaluate on the held-out test set, but also on the validation set and the training set. Report the performance in a 3 by 3 table.\n",
    "\n",
    "Comment on what you see. How do the three different algorithms compare? Also, compare the results between the different dataset splits. In cases like these, one might expect slightly inferior results on the held-out test data compared to the validation and training data. Is this the case? \n",
    "\n",
    "Also report precision and recall for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class task (X, t_multi)\n",
    "\n",
    "Compare the three multi-class classifiers, the one-vs-rest and the multinomial logistic regression from part one and the multi-class neural network from part two. Evaluate on test, validation and training set as above.\n",
    "\n",
    "Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
